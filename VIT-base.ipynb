{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 10:02:02.059828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-24 10:02:03.811738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_1372/3647894667.py:22: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 10:02:07.176987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:07.235453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:07.236004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:08.820606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:08.821181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:08.821222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-24 10:02:08.821574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:08.821639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5836 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 # Change this value as per requirement\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (100, 100, 3)\n",
    "\n",
    "\n",
    "def download_image_buffer(url):\n",
    "    img = None\n",
    "    try:\n",
    "        data = requests.get(url,headers={\"user-agent\": \"I am a valid user, please give me image\"}).content\n",
    "        img = Image.open(io.BytesIO(data))\n",
    "        img = img.resize(image_shape[:2])\n",
    "        img = np.array(img, dtype=np.float64)/255\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def load_img(path, shape):\n",
    "    img = keras.utils.load_img(path, color_mode=\"rgb\")\n",
    "    img = img.resize(shape)\n",
    "    return np.array(img, dtype=np.float64)/255\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "image_size = 100 \n",
    "patch_size = 5 \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 145997.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 30186/30186 [00:00<00:00, 2212249.66it/s]\n"
     ]
    }
   ],
   "source": [
    "_max=100000\n",
    "\n",
    "with open(\"./datasets/conceptual-12m/cc12m.tsv\", \"r\", encoding=\"utf-8\") as dataset_file:\n",
    "    urls, texts = [], []\n",
    "\n",
    "    for i, item in enumerate(tqdm(dataset_file, total=_max)):\n",
    "        if i == _max:\n",
    "            break\n",
    "        url, text = item.split(\"\\t\")\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "image_files = os.listdir(\"./datasets/conceptual-12m/images\")\n",
    "image_idx = [int(item.split(\".\")[0]) for item in tqdm(image_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "captions = []\n",
    "\n",
    "for i, item in enumerate(image_idx):\n",
    "    text = texts[item].replace(\"\\n\", \"\")\n",
    "    captions.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = np.array(image_files, dtype=str)\n",
    "captions = np.array(captions, dtype=str)\n",
    "\n",
    "base_dir = \"./datasets/conceptual-12m/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, text):\n",
    "    path = path.numpy().decode('utf-8')\n",
    "    text = text.numpy().decode('utf-8')\n",
    "\n",
    "    img = load_img(base_dir + path, image_shape[:2])\n",
    "    tokenized_text = np.array(tokenizer.encode(text, max_length=sequence_length, add_special_tokens=True, padding=\"max_length\", truncation=True))\n",
    "\n",
    "    return img, tokenized_text\n",
    "\n",
    "def dict_map(img, tokenized_text):\n",
    "    return ({\n",
    "        \"image\": img,\n",
    "        \"text\": tokenized_text[:-1]\n",
    "    }, tokenized_text[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 10:02:10.811998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.812640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.813263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.814141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.814639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.815066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.815808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.815830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-24 10:02:10.816243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-24 10:02:10.816281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5836 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset = (tf.data.Dataset.from_tensor_slices((image_files, captions))\n",
    "           .shuffle(1000)\n",
    "           .map(lambda filepath, caption: tf.py_function(preprocess, [filepath, caption], [tf.float64, tf.int32]))\n",
    "           .map(dict_map)\n",
    "           .batch(batch_size)\n",
    "           .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches_tensor = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches_tensor.shape[-1]\n",
    "        patches_tensor = tf.reshape(patches_tensor, [batch_size, -1, patch_dims])\n",
    "        return patches_tensor\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dims, hidden_layers, hidden_units=\"same\", attn_dropout=0.2, ff_dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units if hidden_units != \"same\" else embedding_dims\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=attn_dropout\n",
    "        )\n",
    "        self.attn_norm = layers.LayerNormalization()\n",
    "        self.ff = []\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.ff.append(layers.Dense(\n",
    "                self.hidden_units,\n",
    "                activation=tf.nn.gelu\n",
    "            ))\n",
    "            self.ff.append(layers.Dropout(ff_dropout))\n",
    "\n",
    "        self.ff = keras.Sequential(self.ff)\n",
    "        self.ff_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        attn_output = self.attn(\n",
    "            query=inputs,\n",
    "            key=inputs,\n",
    "            value=inputs\n",
    "        )\n",
    "\n",
    "        attn_output_norm = self.attn_norm(inputs + attn_output)\n",
    "\n",
    "        x = self.ff(attn_output_norm)\n",
    "        \n",
    "        ff_output = self.ff_norm(attn_output + x)\n",
    "        \n",
    "        return {\n",
    "            \"ff_output\": ff_output,\n",
    "            \"attn_output\": attn_output\n",
    "        }\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dims,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_hidden_layers\": self.hidden_layers,\n",
    "                \"ff_hidden_units\": self.hidden_units\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_heads_self, \n",
    "                 embedding_dims_self, \n",
    "                 hidden_layers, \n",
    "                 hidden_units=\"same\", \n",
    "                 num_heads_cross=\"same\", \n",
    "                 embedding_dims_cross=\"same\", \n",
    "                 attn_dropout=0.2, \n",
    "                 ff_dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dims = embedding_dims_self\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units if hidden_units != \"same\" else embedding_dims_self\n",
    "        self.num_heads_cross = num_heads_cross if num_heads_cross != \"same\" else num_heads_self\n",
    "        self.embedding_dims_cross = embedding_dims_cross if embedding_dims_cross != \"same\" else embedding_dims_self\n",
    "\n",
    "        self.self_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads_self, \n",
    "            key_dim=embedding_dims_self,\n",
    "            dropout=attn_dropout,\n",
    "            name=\"Self-attn-decoder\"\n",
    "        )\n",
    "        self.self_norm = layers.LayerNormalization()\n",
    "\n",
    "        self.cross_attn = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads_cross, \n",
    "            key_dim=self.embedding_dims_cross,\n",
    "            dropout=attn_dropout,\n",
    "            name=\"cross-attn-decoder\"\n",
    "        )\n",
    "        self.cross_norm = layers.LayerNormalization()\n",
    "        \n",
    "        self.ff = []\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.ff.append(layers.Dense(\n",
    "                self.hidden_units,\n",
    "                activation=tf.nn.gelu\n",
    "            ))\n",
    "            self.ff.append(layers.Dropout(ff_dropout))\n",
    "\n",
    "        self.ff = keras.Sequential(self.ff)\n",
    "        self.ff_norm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()  # instead of `+` to preserve mask\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        self_attn_output = self.self_attn(\n",
    "            query=inputs, key=inputs, value=inputs, use_causal_mask=True\n",
    "        )\n",
    "        self_attn_output = self.self_norm(self.add([inputs, self_attn_output]))\n",
    "\n",
    "        cross_attn_output = self.cross_attn(\n",
    "            query=self_attn_output, key=encoder_outputs, value=encoder_outputs,\n",
    "        )\n",
    "        cross_attn_output = self.cross_norm(self.add([self_attn_output, cross_attn_output]))\n",
    "\n",
    "        ff_output = self.ff(cross_attn_output)\n",
    "        ff_output = self.ff_norm(self.add([ff_output, cross_attn_output]))\n",
    "\n",
    "        return ff_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dims,\n",
    "                \"embedding_dim_cross\": self.embedding_dims_cross,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"num_heads_cross\": self.num_heads_cross,\n",
    "                \"ff_hidden_layers\": self.hidden_layers,\n",
    "                \"ff_hidden_units\": self.hidden_units\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks_encoder = 2\n",
    "num_blocks_decoder = 2\n",
    "num_heads = 8\n",
    "embedding_dims = 64\n",
    "hidden_layers = 2\n",
    "text_embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = layers.Input(shape=image_shape, name=\"image\")\n",
    "patches = Patches(patch_size)(image_input)\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "caption_input = layers.Input(shape=(sequence_length-1), name=\"text\")\n",
    "encoded_captions = PositionalEmbedding(sequence_length, vocab_size, embedding_dims)(caption_input)\n",
    "\n",
    "\n",
    "for _ in range(num_blocks_encoder):\n",
    "    encoded_patches = EncoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_patches)[\"ff_output\"]\n",
    "\n",
    "\n",
    "for _ in range(num_blocks_decoder):\n",
    "    encoded_captions = DecoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_captions, encoded_patches)\n",
    "\n",
    "top_layer = layers.Dense(1024, activation=tf.nn.gelu)(encoded_captions)\n",
    "top_layer = layers.Dense(vocab_size, activation=tf.nn.softmax)(top_layer)\n",
    "\n",
    "test_model = Model(inputs=[image_input, caption_input], outputs=top_layer)\n",
    "test_model.compile(\n",
    "    keras.optimizers.AdamW(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filedfn2ghou.py\", line 13, in tf__call\n        patches_tensor = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches_tensor), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches_6' (type Patches).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_1372/308780364.py\", line 22, in call  *\n            patches_tensor = tf.reshape(patches_tensor, [batch_size, -1, patch_dims])\n    \n        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\n    \n    \n    Call arguments received by layer 'patches_6' (type Patches):\n      • images=tf.Tensor(shape=<unknown>, dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\ML stuff\\VIT_region_approx\\VIT-base.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/d%3A/ML%20stuff/VIT_region_approx/VIT-base.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_model\u001b[39m.\u001b[39;49mpredict(dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filenv29e895.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedfn2ghou.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     11\u001b[0m patches_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mextract_patches, (), \u001b[39mdict\u001b[39m(images\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(images), sizes\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, \u001b[39m1\u001b[39m], strides\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, \u001b[39m1\u001b[39m], rates\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVALID\u001b[39m\u001b[39m'\u001b[39m), fscope)\n\u001b[1;32m     12\u001b[0m patch_dims \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(patches_tensor)\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m patches_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mreshape, (ag__\u001b[39m.\u001b[39;49mld(patches_tensor), [ag__\u001b[39m.\u001b[39;49mld(batch_size), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, ag__\u001b[39m.\u001b[39;49mld(patch_dims)]), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filedfn2ghou.py\", line 13, in tf__call\n        patches_tensor = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches_tensor), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches_6' (type Patches).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_1372/308780364.py\", line 22, in call  *\n            patches_tensor = tf.reshape(patches_tensor, [batch_size, -1, patch_dims])\n    \n        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\n    \n    \n    Call arguments received by layer 'patches_6' (type Patches):\n      • images=tf.Tensor(shape=<unknown>, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " patches (Patches)              (None, None, 75)     0           ['image[0][0]']                  \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 400, 64)      30464       ['patches[0][0]']                \n",
      "                                                                                                  \n",
      " text (InputLayer)              [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " encoder_block (EncoderBlock)   {'ff_output': (None  141248      ['patch_encoder[0][0]']          \n",
      "                                , 400, 64),                                                       \n",
      "                                 'attn_output': (No                                               \n",
      "                                ne, 400, 64)}                                                     \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, 128, 64)     1961600     ['text[0][0]']                   \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " encoder_block_1 (EncoderBlock)  {'ff_output': (None  141248     ['encoder_block[0][1]']          \n",
      "                                , 400, 64),                                                       \n",
      "                                 'attn_output': (No                                               \n",
      "                                ne, 400, 64)}                                                     \n",
      "                                                                                                  \n",
      " decoder_block (DecoderBlock)   (None, 128, 64)      274048      ['positional_embedding[0][0]',   \n",
      "                                                                  'encoder_block_1[0][1]']        \n",
      "                                                                                                  \n",
      " decoder_block_1 (DecoderBlock)  (None, 128, 64)     274048      ['decoder_block[0][0]',          \n",
      "                                                                  'encoder_block_1[0][1]']        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 128, 1024)    66560       ['decoder_block_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128, 30522)   31285050    ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,174,266\n",
      "Trainable params: 34,174,266\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_input = layers.Input(shape=image_shape, name=\"image\")\n",
    "caption_input = layers.Input(shape=(sequence_length), name=\"text\")\n",
    "\n",
    "patches = Patches(patch_size)(image_input)\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "encoded_captions = PositionalEmbedding(sequence_length, vocab_size, embedding_dims)(caption_input)\n",
    "\n",
    "attn_weights = []\n",
    "\n",
    "for _ in range(num_blocks_encoder):\n",
    "    encoded_patches = EncoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_patches)\n",
    "\n",
    "    attn_weights.append(encoded_patches[\"attn_output\"])\n",
    "    encoded_patches = encoded_patches[\"ff_output\"]\n",
    "\n",
    "\n",
    "for _ in range(num_blocks_decoder):\n",
    "    encoded_captions = DecoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_captions, encoded_patches)\n",
    "\n",
    "# top_layer = layers.Flatten()(encoded_captions)\n",
    "top_layer = layers.Dense(1024, activation=tf.nn.gelu)(encoded_captions)\n",
    "top_layer = layers.Dense(vocab_size, activation=tf.nn.softmax)(top_layer)\n",
    "\n",
    "transformer = Model(inputs=[image_input, caption_input], outputs=top_layer, name=\"transformer\")\n",
    "transformer.compile(\n",
    "    keras.optimizers.AdamW(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 09:49:30.969144: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [30186]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-09-24 09:49:30.969634: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [30186]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file58u_9tt3.py\", line 13, in tf__call\n        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches' (type Patches).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_972/2459117379.py\", line 22, in call  *\n            patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n    \n        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\n    \n    \n    Call arguments received by layer 'patches' (type Patches):\n      • images=tf.Tensor(shape=<unknown>, dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\ML stuff\\VIT_region_approx\\VIT-base.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/d%3A/ML%20stuff/VIT_region_approx/VIT-base.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit(dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filescy7qmpa.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file58u_9tt3.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     11\u001b[0m patches \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mextract_patches, (), \u001b[39mdict\u001b[39m(images\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(images), sizes\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, \u001b[39m1\u001b[39m], strides\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpatch_size, \u001b[39m1\u001b[39m], rates\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVALID\u001b[39m\u001b[39m'\u001b[39m), fscope)\n\u001b[1;32m     12\u001b[0m patch_dims \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(patches)\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m patches \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mreshape, (ag__\u001b[39m.\u001b[39;49mld(patches), [ag__\u001b[39m.\u001b[39;49mld(batch_size), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, ag__\u001b[39m.\u001b[39;49mld(patch_dims)]), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/root/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file58u_9tt3.py\", line 13, in tf__call\n        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches' (type Patches).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_972/2459117379.py\", line 22, in call  *\n            patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n    \n        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\n    \n    \n    Call arguments received by layer 'patches' (type Patches):\n      • images=tf.Tensor(shape=<unknown>, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
