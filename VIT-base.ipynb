{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 18:48:19.891636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-23 18:48:19.894432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-23 18:48:19.895142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-23 18:48:19.918307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-23 18:48:19.918518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-23 18:48:19.919089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-23 18:48:19.919707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5836 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 # Change this value as per requirement\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (100, 100, 3)\n",
    "\n",
    "\n",
    "def download_image_buffer(url):\n",
    "    img = None\n",
    "    try:\n",
    "        data = requests.get(url,headers={\"user-agent\": \"I am a valid user, please give me image\"}).content\n",
    "        img = Image.open(io.BytesIO(data))\n",
    "        img = img.resize(image_shape[:2])\n",
    "        img = np.array(img, dtype=np.float64)/255\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "image_size = 100 \n",
    "patch_size = 5 \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 100000/100000 [00:01<00:00, 76815.62it/s]\n"
     ]
    }
   ],
   "source": [
    "_max=100000\n",
    "\n",
    "with open(\"./datasets/conceptual-12m/cc12m.tsv\", \"r\", encoding=\"utf-8\") as dataset_file:\n",
    "    urls, texts = [], []\n",
    "\n",
    "    for i, item in enumerate(tqdm(dataset_file, total=_max)):\n",
    "        if i == _max:\n",
    "            break\n",
    "        url, text = item.split(\"\\t\")\n",
    "\n",
    "        urls.append(url)\n",
    "        texts.append(text)\n",
    "\n",
    "urls = np.array(urls)\n",
    "texts = np.array(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(url, text):\n",
    "    try:\n",
    "        url = url.numpy().decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        text = text.numpy().decode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    img = download_image_buffer(url)\n",
    "\n",
    "    # print(img is not None)\n",
    "    \n",
    "    if img is None:\n",
    "        text = \"A blank image\"\n",
    "        img = np.zeros(image_shape, dtype=np.float64)\n",
    "\n",
    "    tokenized_text = np.array(tokenizer.encode(text))[:sequence_length]\n",
    "\n",
    "    return img, tokenized_text\n",
    "\n",
    "def dict_map(img, tokenized_text):\n",
    "    return ({\n",
    "        \"image\": img,\n",
    "        \"text\": tokenized_text[:-1]\n",
    "    }, tokenized_text[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20/20 [00:10<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "a, b = [], []\n",
    "\n",
    "for u, t in tqdm(zip(urls[:20], texts[:20]), total=20):\n",
    "    img, tokenized = preprocess(u, t)\n",
    "\n",
    "    a.append(img)\n",
    "    b.append(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (tf.data.Dataset.from_tensor_slices((urls, texts))\n",
    "           .shuffle(1000)\n",
    "           .map(lambda url, text: tf.py_function(preprocess, [url, text], [tf.float64, tf.int32]))\n",
    "        #    .map(dict_map)\n",
    "           .batch(batch_size)\n",
    "         #   .prefetch(tf.data.AUTOTUNE)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 17:29:54.831366: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [100000]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-09-23 17:29:54.831786: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [100000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [5], [batch]: [8] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\ML stuff\\VIT_region_approx\\VIT-base.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/d%3A/ML%20stuff/VIT_region_approx/VIT-base.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset\u001b[39m.\u001b[39;49mas_numpy_iterator()\u001b[39m.\u001b[39;49mnext()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:4665\u001b[0m, in \u001b[0;36m_NumpyIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4664\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 4665\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__next__\u001b[39;49m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:4662\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4659\u001b[0m     numpy\u001b[39m.\u001b[39msetflags(write\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   4660\u001b[0m   \u001b[39mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4662\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(to_numpy, \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3015\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 3016\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3017\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   3018\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [5], [batch]: [8] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dims, hidden_layers, hidden_units=\"same\", attn_dropout=0.2, ff_dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units if hidden_units != \"same\" else embedding_dims\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=attn_dropout\n",
    "        )\n",
    "        self.attn_norm = layers.LayerNormalization()\n",
    "        self.ff = []\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.ff.append(layers.Dense(\n",
    "                self.hidden_units,\n",
    "                activation=tf.nn.gelu\n",
    "            ))\n",
    "            self.ff.append(layers.Dropout(ff_dropout))\n",
    "\n",
    "        self.ff = keras.Sequential(self.ff)\n",
    "        self.ff_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        attn_output = self.attn(\n",
    "            query=inputs,\n",
    "            key=inputs,\n",
    "            value=inputs\n",
    "        )\n",
    "\n",
    "        attn_output_norm = self.attn_norm(inputs + attn_output)\n",
    "\n",
    "        x = self.ff(attn_output_norm)\n",
    "        \n",
    "        ff_output = self.ff_norm(attn_output + x)\n",
    "\n",
    "        return {\n",
    "            \"ff_output\": ff_output,\n",
    "            \"attn_output\": attn_output\n",
    "        }\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dims,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_hidden_layers\": self.hidden_layers,\n",
    "                \"ff_hidden_units\": self.hidden_units\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_heads_self, \n",
    "                 embedding_dims_self, \n",
    "                 hidden_layers, \n",
    "                 hidden_units=\"same\", \n",
    "                 num_heads_cross=\"same\", \n",
    "                 embedding_dims_cross=\"same\", \n",
    "                 attn_dropout=0.2, \n",
    "                 ff_dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dims = embedding_dims_self\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units if hidden_units != \"same\" else embedding_dims_self\n",
    "        self.num_heads_cross = num_heads_cross if num_heads_cross != \"same\" else num_heads_self\n",
    "        self.embedding_dims_cross = embedding_dims_cross if embedding_dims_cross != \"same\" else embedding_dims_self\n",
    "\n",
    "        self.self_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads_self, \n",
    "            key_dim=embedding_dims_self,\n",
    "            dropout=attn_dropout,\n",
    "            name=\"Self-attn-decoder\"\n",
    "        )\n",
    "        self.self_norm = layers.LayerNormalization()\n",
    "\n",
    "        self.cross_attn = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads_cross, \n",
    "            key_dim=self.embedding_dims_cross,\n",
    "            dropout=attn_dropout,\n",
    "            name=\"cross-attn-decoder\"\n",
    "        )\n",
    "        self.cross_norm = layers.LayerNormalization()\n",
    "        \n",
    "        self.ff = []\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.ff.append(layers.Dense(\n",
    "                self.hidden_units,\n",
    "                activation=tf.nn.gelu\n",
    "            ))\n",
    "            self.ff.append(layers.Dropout(ff_dropout))\n",
    "\n",
    "        self.ff = keras.Sequential(self.ff)\n",
    "        self.ff_norm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()  # instead of `+` to preserve mask\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        self_attn_output = self.self_attn(\n",
    "            query=inputs, key=inputs, value=inputs, use_causal_mask=True\n",
    "        )\n",
    "        self_attn_output = self.self_norm(self.add([inputs, self_attn_output]))\n",
    "\n",
    "        cross_attn_output = self.cross_attn(\n",
    "            query=self_attn_output, key=encoder_outputs, value=encoder_outputs,\n",
    "        )\n",
    "        cross_attn_output = self.cross_norm(self.add([self_attn_output, cross_attn_output]))\n",
    "\n",
    "        ff_output = self.ff(cross_attn_output)\n",
    "        ff_output = self.ff_norm(self.add([ff_output, cross_attn_output]))\n",
    "\n",
    "        return ff_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dims,\n",
    "                \"embedding_dim_cross\": self.embedding_dims_cross,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"num_heads_cross\": self.num_heads_cross,\n",
    "                \"ff_hidden_layers\": self.hidden_layers,\n",
    "                \"ff_hidden_units\": self.hidden_units\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks_encoder = 2\n",
    "num_blocks_decoder = 2\n",
    "num_heads = 8\n",
    "embedding_dims = 64\n",
    "hidden_layers = 2\n",
    "sequence_length = 128\n",
    "text_embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = layers.Input(shape=image_shape, name=\"image\")\n",
    "caption_input = layers.Input(shape=(sequence_length), name=\"text\")\n",
    "\n",
    "patches = Patches(patch_size)(image_input)\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "encoded_captions = PositionalEmbedding(sequence_length, vocab_size, embedding_dims)(caption_input)\n",
    "\n",
    "attn_weights = []\n",
    "\n",
    "for _ in range(num_blocks_encoder):\n",
    "    encoded_patches = EncoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_patches)\n",
    "\n",
    "    attn_weights.append(encoded_patches[\"attn_output\"])\n",
    "    encoded_patches = encoded_patches[\"ff_output\"]\n",
    "\n",
    "\n",
    "for _ in range(num_blocks_decoder):\n",
    "    encoded_captions = DecoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_captions, encoded_patches)\n",
    "\n",
    "# top_layer = layers.Flatten()(encoded_captions)\n",
    "top_layer = layers.Dense(1024, activation=tf.nn.gelu)(encoded_captions)\n",
    "top_layer = layers.Dense(vocab_size, activation=tf.nn.softmax)(top_layer)\n",
    "\n",
    "transformer = Model(inputs=[image_input, caption_input], outputs=top_layer, name=\"transformer\")\n",
    "transformer.compile(\n",
    "    keras.optimizers.AdamW(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
