{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm tensorflow-hub tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    jsons = []\n",
    "\n",
    "    for json_str in json_list:\n",
    "        jsons.append(json.loads(json_str))\n",
    "\n",
    "    return jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./MASSIVE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filelist = os.listdir(f\"{root_dir}/train_data\")\n",
    "test_filelist = os.listdir(f\"{root_dir}/test_data\")\n",
    "len(train_filelist), len(test_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_scenario = []\n",
    "train_labels = []\n",
    "\n",
    "for item in train_filelist:\n",
    "    data = load_jsonl(f\"{root_dir}/train_data/{item}\")\n",
    "\n",
    "    for example in tqdm(data, f\"loading {item}\"):\n",
    "        train_texts.append(example[\"utt\"])\n",
    "        train_scenario.append(example[\"scenario\"])\n",
    "        train_labels.append(example[\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = Series(train_labels).unique()\n",
    "idx = Series(Series(train_labels).unique()).index\n",
    "mapping = Series(idx, index=unq)\n",
    "train_labels = to_categorical([mapping[item] for item in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = Series(train_scenario).unique()\n",
    "idx = Series(Series(train_scenario).unique()).index\n",
    "mapping = Series(idx, index=unq)\n",
    "scenarios = [mapping[item] for item in train_scenario]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # Needed for loading universal-sentence-encoder-cmlm/multilingual-preprocess\n",
    "import numpy as np\n",
    "\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1\", trainable=False)\n",
    "\n",
    "# english_embeds = encoder(preprocessor(english_sentences))[\"default\"]\n",
    "# japanese_embeds = encoder(preprocessor(japanese_sentences))[\"default\"]\n",
    "# italian_embeds = encoder(preprocessor(italian_sentences))[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_map(texts, scenario, labels):\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"scenario\": scenario\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = (tf.data.Dataset.from_tensor_slices((train_texts, scenarios, train_labels)).map(dataset_map)\n",
    "            .shuffle(buffer_size=1000)\n",
    "            .batch(BATCH_SIZE))\n",
    "\n",
    "DATASET_SIZE = len(dataset)\n",
    "\n",
    "split = 0.8\n",
    "\n",
    "train_size = int(split * DATASET_SIZE)\n",
    "val_size = int(1-split * DATASET_SIZE)\n",
    "\n",
    "train_ds = dataset.take(train_size)\n",
    "validation_ds = dataset.skip(train_size)\n",
    "\n",
    "len(train_ds), len(validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "scenario_input = layers.Input(shape=(1), dtype=tf.int32, name=\"scenario\")\n",
    "scenario_encoder = layers.Embedding(60, 256)(scenario_input)\n",
    "scenario_encoder = layers.Flatten()(scenario_encoder)\n",
    "\n",
    "x = preprocessor(text_input)\n",
    "x = encoder(x)[\"default\"]\n",
    "\n",
    "x = layers.Concatenate()([x, scenario_encoder])\n",
    "\n",
    "x = layers.Dense(train_labels.shape[-1], activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[text_input, scenario_input], outputs=x)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint: Save the model's weights after every epoch\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./checkpoints/model_checkpoint.h5', save_best_only=True)\n",
    "# EarlyStopping: Stop training when a monitored metric has stopped improving\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=validation_ds, epochs=5, callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
