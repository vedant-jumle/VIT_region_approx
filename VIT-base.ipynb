{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95 # Change this value as per requirement\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (100, 100, 3)\n",
    "\n",
    "\n",
    "def download_image_buffer(url):\n",
    "    img = None\n",
    "    try:\n",
    "        data = requests.get(url,headers={\"user-agent\": \"I am a valid user, please give me image\"}).content\n",
    "        img = Image.open(io.BytesIO(data))\n",
    "        img = img.resize(image_shape[:2])\n",
    "        img = np.array(img, dtype=np.float64)/255\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def load_img(path, shape):\n",
    "    img = keras.utils.load_img(path, color_mode=\"rgb\")\n",
    "    img = img.resize(shape)\n",
    "    return np.array(img, dtype=np.float64)/255\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "image_size = 100 \n",
    "patch_size = 5 \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 178500.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30186/30186 [00:00<00:00, 1888760.17it/s]\n"
     ]
    }
   ],
   "source": [
    "_max=100000\n",
    "\n",
    "with open(\"./datasets/conceptual-12m/cc12m.tsv\", \"r\", encoding=\"utf-8\") as dataset_file:\n",
    "    urls, texts = [], []\n",
    "\n",
    "    for i, item in enumerate(tqdm(dataset_file, total=_max)):\n",
    "        if i == _max:\n",
    "            break\n",
    "        url, text = item.split(\"\\t\")\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "image_files = os.listdir(\"./datasets/conceptual-12m/images\")\n",
    "image_idx = [int(item.split(\".\")[0]) for item in tqdm(image_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "captions = []\n",
    "\n",
    "for i, item in enumerate(image_idx):\n",
    "    text = texts[item].replace(\"\\n\", \"\")\n",
    "    captions.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = np.array(image_files, dtype=str)\n",
    "captions = np.array(captions, dtype=str)\n",
    "\n",
    "base_dir = \"./datasets/conceptual-12m/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, text):\n",
    "    path = path.numpy().decode('utf-8')\n",
    "    text = text.numpy().decode('utf-8')\n",
    "\n",
    "    img = load_img(base_dir + path, image_shape[:2])\n",
    "    tokenized_text = np.array(tokenizer.encode(text, max_length=sequence_length, add_special_tokens=True, padding=\"max_length\", truncation=True))\n",
    "\n",
    "    return img, tokenized_text\n",
    "\n",
    "def dict_map(img, tokenized_text):\n",
    "    return ({\n",
    "        \"image\": img,\n",
    "        \"text\": tokenized_text[:-1]\n",
    "    }, tokenized_text[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (tf.data.Dataset.from_tensor_slices((image_files, captions))\n",
    "           .shuffle(1000)\n",
    "           .map(lambda filepath, caption: tf.py_function(preprocess, [filepath, caption], [tf.float64, tf.int32]))\n",
    "           .map(dict_map)\n",
    "           .batch(batch_size)\n",
    "           .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, num_patches, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        self._input = images\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches_tensor = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches_tensor.shape[-1]  # Dimension of each patch\n",
    "\n",
    "        patches_tensor = tf.reshape(patches_tensor, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches_tensor\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self._input = inputs\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, embedding_dims, hidden_layers, hidden_units=\"same\", attn_dropout=0.2, ff_dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units if hidden_units != \"same\" else embedding_dims\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=attn_dropout\n",
    "        )\n",
    "        self.attn_norm = layers.LayerNormalization()\n",
    "        self.ff = []\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.ff.append(layers.Dense(\n",
    "                self.hidden_units,\n",
    "                activation=tf.nn.gelu\n",
    "            ))\n",
    "            self.ff.append(layers.Dropout(ff_dropout))\n",
    "\n",
    "        self.ff = keras.Sequential(self.ff)\n",
    "        self.ff_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        attn_output = self.attn(\n",
    "            query=inputs,\n",
    "            key=inputs,\n",
    "            value=inputs\n",
    "        )\n",
    "\n",
    "        attn_output_norm = self.attn_norm(inputs + attn_output)\n",
    "\n",
    "        x = self.ff(attn_output_norm)\n",
    "        \n",
    "        ff_output = self.ff_norm(attn_output + x)\n",
    "        \n",
    "        return {\n",
    "            \"ff_output\": ff_output,\n",
    "            \"attn_output\": attn_output\n",
    "        }\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dims,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_hidden_layers\": self.hidden_layers,\n",
    "                \"ff_hidden_units\": self.hidden_units\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_heads_self, \n",
    "                 embedding_dims_self, \n",
    "                 hidden_layers, \n",
    "                 hidden_units=\"same\", \n",
    "                 num_heads_cross=\"same\", \n",
    "                 embedding_dims_cross=\"same\", \n",
    "                 attn_dropout=0.2, \n",
    "                 ff_dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dims = embedding_dims_self\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_units = hidden_units if hidden_units != \"same\" else embedding_dims_self\n",
    "        self.num_heads_cross = num_heads_cross if num_heads_cross != \"same\" else num_heads_self\n",
    "        self.embedding_dims_cross = embedding_dims_cross if embedding_dims_cross != \"same\" else embedding_dims_self\n",
    "\n",
    "        self.self_attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads_self, \n",
    "            key_dim=embedding_dims_self,\n",
    "            dropout=attn_dropout,\n",
    "            name=\"Self-attn-decoder\"\n",
    "        )\n",
    "        self.self_norm = layers.LayerNormalization()\n",
    "\n",
    "        self.cross_attn = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads_cross, \n",
    "            key_dim=self.embedding_dims_cross,\n",
    "            dropout=attn_dropout,\n",
    "            name=\"cross-attn-decoder\"\n",
    "        )\n",
    "        self.cross_norm = layers.LayerNormalization()\n",
    "        \n",
    "        self.ff = []\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            self.ff.append(layers.Dense(\n",
    "                self.hidden_units,\n",
    "                activation=tf.nn.gelu\n",
    "            ))\n",
    "            self.ff.append(layers.Dropout(ff_dropout))\n",
    "\n",
    "        self.ff = keras.Sequential(self.ff)\n",
    "        self.ff_norm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()  # instead of `+` to preserve mask\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        self_attn_output = self.self_attn(\n",
    "            query=inputs, key=inputs, value=inputs, use_causal_mask=True\n",
    "        )\n",
    "        self_attn_output = self.self_norm(self.add([inputs, self_attn_output]))\n",
    "\n",
    "        cross_attn_output = self.cross_attn(\n",
    "            query=self_attn_output, key=encoder_outputs, value=encoder_outputs,\n",
    "        )\n",
    "        cross_attn_output = self.cross_norm(self.add([self_attn_output, cross_attn_output]))\n",
    "\n",
    "        ff_output = self.ff(cross_attn_output)\n",
    "        ff_output = self.ff_norm(self.add([ff_output, cross_attn_output]))\n",
    "\n",
    "        return ff_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\": self.embedding_dims,\n",
    "                \"embedding_dim_cross\": self.embedding_dims_cross,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"num_heads_cross\": self.num_heads_cross,\n",
    "                \"ff_hidden_layers\": self.hidden_layers,\n",
    "                \"ff_hidden_units\": self.hidden_units\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks_encoder = 2\n",
    "num_blocks_decoder = 2\n",
    "num_heads = 8\n",
    "embedding_dims = 64\n",
    "hidden_layers = 2\n",
    "text_embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " patches (Patches)              (None, 400, 75)      0           ['image[0][0]']                  \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 400, 64)      30464       ['patches[0][0]']                \n",
      "                                                                                                  \n",
      " text (InputLayer)              [(None, 127)]        0           []                               \n",
      "                                                                                                  \n",
      " encoder_block (EncoderBlock)   {'ff_output': (None  141248      ['patch_encoder[0][0]']          \n",
      "                                , 400, 64),                                                       \n",
      "                                 'attn_output': (No                                               \n",
      "                                ne, 400, 64)}                                                     \n",
      "                                                                                                  \n",
      " caption_encoder (PositionalEmb  (None, 127, 64)     1961600     ['text[0][0]']                   \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " encoder_block_1 (EncoderBlock)  {'ff_output': (None  141248     ['encoder_block[0][1]']          \n",
      "                                , 400, 64),                                                       \n",
      "                                 'attn_output': (No                                               \n",
      "                                ne, 400, 64)}                                                     \n",
      "                                                                                                  \n",
      " decoder_block (DecoderBlock)   (None, 127, 64)      274048      ['caption_encoder[0][0]',        \n",
      "                                                                  'encoder_block_1[0][1]']        \n",
      "                                                                                                  \n",
      " decoder_block_1 (DecoderBlock)  (None, 127, 64)     274048      ['decoder_block[0][0]',          \n",
      "                                                                  'encoder_block_1[0][1]']        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 127, 1024)    66560       ['decoder_block_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 127, 30522)   31285050    ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,174,266\n",
      "Trainable params: 34,174,266\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_input = layers.Input(shape=image_shape, name=\"image\")\n",
    "patches = Patches(patch_size, num_patches, name=\"patches\")(image_input)\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim, name=\"patch_encoder\")(patches)\n",
    "\n",
    "caption_input = layers.Input(shape=(sequence_length-1), name=\"text\")\n",
    "encoded_captions = PositionalEmbedding(sequence_length, vocab_size, embedding_dims, name=\"caption_encoder\")(caption_input)\n",
    "\n",
    "\n",
    "for _ in range(num_blocks_encoder):\n",
    "    encoded_patches = EncoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_patches)[\"ff_output\"]\n",
    "\n",
    "\n",
    "for _ in range(num_blocks_decoder):\n",
    "    encoded_captions = DecoderBlock(\n",
    "        num_heads,\n",
    "        embedding_dims,\n",
    "        hidden_layers\n",
    "    )(encoded_captions, encoded_patches)\n",
    "\n",
    "top_layer = layers.Dense(1024, activation=tf.nn.gelu)(encoded_captions)\n",
    "top_layer = layers.Dense(vocab_size, activation=tf.nn.softmax)(top_layer)\n",
    "\n",
    "test_model = Model(inputs=[image_input, caption_input], outputs=top_layer)\n",
    "test_model.compile(\n",
    "    keras.optimizers.Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17/944 [..............................] - ETA: 14:40 - loss: 8.7184 - accuracy: 0.0414"
     ]
    }
   ],
   "source": [
    "progbar = Progbar(len(dataset))\n",
    "\n",
    "dataset_iter = dataset.as_numpy_iterator()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for step in range(len(dataset)):\n",
    "\n",
    "        batch_input, target = dataset_iter.next()\n",
    "        result = test_model.train_on_batch(batch_input, y=target, return_dict=True)\n",
    "\n",
    "        loss = result[\"loss\"]\n",
    "        accuracy = result['accuracy']\n",
    "\n",
    "        progbar.update(step + 1, [('loss', loss), ('accuracy', accuracy)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
